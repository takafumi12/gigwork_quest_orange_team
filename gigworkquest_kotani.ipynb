{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# digitsデータを使うためにimportする\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kttn6\\Google ドライブ\\AI_Quest\\gigu_quest_1\n"
     ]
    }
   ],
   "source": [
    "cd \"C:/Users/kttn6/Google ドライブ/AI_Quest/gigu_quest_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:18<00:00, 2737.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from skimage import io\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "#学習データの画像読み込み\n",
    "file_list = glob.glob(\"./train/*.png\")\n",
    "image_list = []\n",
    "\n",
    "for file in tqdm(file_list[0:50000]):\n",
    "    #image = cv2.resize(io.imread(file), (224,224))\n",
    "    image = cv2.imread(file)\n",
    "    image = np.transpose(image, (2,0,1))\n",
    "    #image = np.array(image)\n",
    "    image_list.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1, 32, 32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list[:,:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image_list.reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iwaさんコード参考\n",
    "PATH = 'C:/Users/kttn6/Google ドライブ/AI_Quest/'\n",
    "\n",
    "import glob\n",
    "import os.path as osp\n",
    "import torch.utils.data as data\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ImageTransform():\n",
    "    \"\"\"\n",
    "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
    "    画像のサイズをリサイズし、色を標準化する。\n",
    "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    resize : int\n",
    "        リサイズ先の画像の大きさ。\n",
    "    mean : (R, G, B)\n",
    "        各色チャネルの平均値。\n",
    "    std : (R, G, B)\n",
    "        各色チャネルの標準偏差。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resize, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train': transforms.Compose([\n",
    "                # transforms.RandomResizedCrop(\n",
    "                #     resize, scale=(0.5, 1.0)),  # データオーギュメンテーション\n",
    "                # transforms.RandomHorizontalFlip(),  # データオーギュメンテーション\n",
    "                transforms.CenterCrop(resize),  # 画像中央をresize×resizeで切り取り\n",
    "                transforms.ToTensor(),  # テンソルに変換\n",
    "                transforms.Normalize(mean, std)  # 標準化\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(resize),  # リサイズ\n",
    "                transforms.CenterCrop(resize),  # 画像中央をresize×resizeで切り取り\n",
    "                transforms.ToTensor(),  # テンソルに変換\n",
    "                transforms.Normalize(mean, std)  # 標準化\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    def __call__(self, img, phase='train'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        phase : 'train' or 'val'\n",
    "            前処理のモードを指定。\n",
    "        \"\"\"\n",
    "        return self.data_transform[phase](img)\n",
    "\n",
    "\n",
    "def make_datapath_list(phase=\"train\"):\n",
    "    \"\"\"\n",
    "    データのパスを格納したリストを作成する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phase : 'train' or 'val'\n",
    "        訓練データか検証データかを指定する\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path_list : list\n",
    "        データへのパスを格納したリスト\n",
    "    \"\"\"\n",
    "\n",
    "    rootpath = PATH + \"gigu_quest_1/\"\n",
    "    target_path = osp.join(rootpath+phase+'/*.png')\n",
    "    print(target_path)\n",
    "\n",
    "    path_list = []  # ここに格納する\n",
    "\n",
    "    # globを利用してサブディレクトリまでファイルパスを取得する\n",
    "    for path in glob.glob(target_path):\n",
    "        path_list.append(path)\n",
    "\n",
    "    return path_list\n",
    "\n",
    "\n",
    "class HymenopteraDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    画像のDatasetクラス。PyTorchのDatasetクラスを継承。\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    file_list : リスト\n",
    "        画像のパスを格納したリスト\n",
    "    transform : object\n",
    "        前処理クラスのインスタンス\n",
    "    phase : 'train' or 'test'\n",
    "        学習か訓練かを設定する。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_list, y, transform=None, phase='train'):\n",
    "        self.file_list = file_list  # ファイルパスのリスト\n",
    "        self.y = y #画像の正解ラベル\n",
    "        self.transform = transform  # 前処理クラスのインスタンス\n",
    "        self.phase = phase  # train or valの指定\n",
    "\n",
    "    def __len__(self):\n",
    "        '''画像の枚数を返す'''\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        前処理をした画像のTensor形式のデータとラベルを取得\n",
    "        '''\n",
    "\n",
    "        # index番目の画像をロード\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)  # [高さ][幅][色RGB]\n",
    "\n",
    "        # 画像の前処理を実施\n",
    "        img_transformed = self.transform(\n",
    "            img, self.phase)  # torch.Size([3, 224, 224])\n",
    "\n",
    "        return img_transformed, y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 15,  4, ...,  8,  7,  1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学習データのラベルデータ読み込み\n",
    "df = pd.read_csv('./train_master.tsv',sep='\\t')\n",
    "y = np.array(df[\"label_id\"])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/kttn6/Google ドライブ/AI_Quest/gigu_quest_1/train/*.png\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "# 1.3節で作成したクラスを同じフォルダにあるmake_dataset_dataloader.pyに記載して使用\n",
    "#from utils.dataloader_image_classification import ImageTransform, make_datapath_list, HymenopteraDataset\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('/content/drive/My Drive/Colab/my_project/my_modules')\n",
    "\n",
    "# 画像へのファイルパスのリストを作成する\n",
    "train_list = make_datapath_list(phase=\"train\")\n",
    "# test_list = make_datapath_list(phase=\"test\")\n",
    "\n",
    "# Datasetを作成する\n",
    "size = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = HymenopteraDataset(\n",
    "    file_list=train_list, y=y, transform=ImageTransform(size, mean, std), phase='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(train_dataset)\n",
    "train_size = int(len(train_dataset) * 0.8)\n",
    "val_size = n_samples - train_size\n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset),len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 転移学習へ進む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7223aea0bd1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Tensor型に変換\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Tensor型に変換\n",
    "X = torch.tensor(image_list, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-fe607b227c30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Datasetの作成\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "#Datasetの作成\n",
    "trainval_dataset = torch.utils.data.TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_testに分割\n",
    "n_samples = len(trainval_dataset)\n",
    "train_size = int(len(trainval_dataset) * 0.8)\n",
    "val_size = n_samples - train_size\n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "n_samples = len(trainval_dataset) # n_samples is 60000\n",
    "train_size = int(n_samples * 0.8) # train_size is 48000\n",
    "\n",
    "subset1_indices = list(range(0,train_size)) # [0,1,.....47999]\n",
    "subset2_indices = list(range(train_size,n_samples)) # [48000,48001,.....59999]\n",
    "\n",
    "trainset = Subset(trainval_dataset, subset1_indices)\n",
    "testset   = Subset(trainval_dataset, subset2_indices)\n",
    "\n",
    "print(len(trainset)) # 48000\n",
    "print(len(testset)) # 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = tuple([i for i in range(20)])\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 転移学習\n",
    "---\n",
    "今回はVGG16を利用しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "model_ft2 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメーターを学習させないようにしている\n",
    "# Enter your code here\n",
    "for param in model_ft2.parameters():\n",
    "    param.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後の一層だけ書き換えて、最後の一層だけ学習させる\n",
    "# Enter your code here\n",
    "model_ft2.fc = nn.Linear(in_features=512, out_features=len(classes), bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "print(loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model_ft2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft2 = model_ft2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1\n",
      "Pred=tensor([[-1.1862,  0.2604, -1.9333,  ..., -1.7299,  0.8436,  0.9212],\n",
      "        [-0.3256,  1.6480, -0.9734,  ..., -1.4681,  0.5530,  0.1676],\n",
      "        [-1.3752,  1.2628, -1.3629,  ..., -1.7871, -0.1099,  0.5304],\n",
      "        ...,\n",
      "        [-0.4499,  1.8054, -2.0232,  ..., -1.0767,  1.1758,  0.1061],\n",
      "        [-1.4916,  0.7220, -2.5096,  ..., -1.9371,  1.1820,  0.6104],\n",
      "        [ 0.0978,  1.4153, -1.4650,  ..., -1.0984,  1.2834, -0.7878]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step2\n",
      "Pred=tensor([[-1.7887,  1.5471, -1.4597,  ..., -1.2858,  0.7582,  1.0331],\n",
      "        [-1.0407,  0.9261, -0.5611,  ..., -0.6696,  0.8495,  0.4568],\n",
      "        [-1.6561,  0.4278, -0.3513,  ..., -0.7516, -0.0926,  0.5404],\n",
      "        ...,\n",
      "        [-1.9640,  1.2012, -1.0064,  ..., -1.2520,  0.3237,  2.0046],\n",
      "        [-0.6117,  1.4289,  0.4959,  ..., -0.8092,  0.2435,  0.3311],\n",
      "        [-0.9588,  1.5859, -0.6434,  ..., -1.4679,  0.2523,  0.6877]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step3\n",
      "Pred=tensor([[-1.6868,  2.0165, -0.7049,  ..., -1.3486,  1.4033,  1.0300],\n",
      "        [-1.6582,  1.7949, -0.8931,  ..., -0.4865,  1.3827,  0.6220],\n",
      "        [-1.9926,  2.0597, -0.9773,  ..., -0.8837,  0.8408,  1.8471],\n",
      "        ...,\n",
      "        [-1.1335,  1.1868, -0.7611,  ...,  0.1688,  0.7570,  0.7879],\n",
      "        [-1.4545,  1.6943, -1.1781,  ..., -0.5285,  1.5656,  0.6436],\n",
      "        [-1.4550,  1.3770, -1.0633,  ..., -0.0253,  1.2203,  0.0816]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step4\n",
      "Pred=tensor([[-2.5322,  1.1770, -1.2781,  ..., -0.1892,  1.2182,  0.7821],\n",
      "        [-1.7363,  1.7716, -0.9026,  ...,  0.1716,  0.8408,  0.9061],\n",
      "        [-2.0731,  2.4731, -1.2204,  ..., -0.9138,  1.1251,  0.3662],\n",
      "        ...,\n",
      "        [-1.4246,  1.0818, -1.6804,  ...,  1.1194,  1.3037,  0.4995],\n",
      "        [-0.8135,  1.2901, -1.0984,  ...,  0.0427,  0.3292,  0.6790],\n",
      "        [-1.3696,  1.8438, -1.3974,  ...,  0.3413,  0.7826,  1.2237]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step5\n",
      "Pred=tensor([[-1.8099,  1.4579, -0.5170,  ..., -0.5511,  0.1968,  1.3500],\n",
      "        [-1.8548,  1.1410, -0.0563,  ...,  0.3341,  1.2505,  0.6874],\n",
      "        [-1.6631,  1.9096, -0.7785,  ..., -0.1987,  0.3734,  0.7714],\n",
      "        ...,\n",
      "        [-1.9450,  1.8181, -0.6206,  ..., -0.6918,  0.4792,  0.9515],\n",
      "        [-2.1535,  1.8332, -0.6399,  ...,  0.2125,  0.1569,  1.4623],\n",
      "        [-2.5630,  1.9148, -0.5807,  ..., -0.4105,  0.6282,  1.0905]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step6\n",
      "Pred=tensor([[-1.5387,  2.8000, -0.3515,  ..., -0.0184,  0.8438,  0.6264],\n",
      "        [-2.0524,  1.7944, -0.8246,  ...,  0.0946,  0.9405,  0.9192],\n",
      "        [-1.3841,  1.6946, -0.4855,  ...,  0.0586,  1.1527,  0.8270],\n",
      "        ...,\n",
      "        [-1.3699,  1.1043, -0.6323,  ..., -0.3608, -0.0227,  1.0688],\n",
      "        [-0.8900,  2.2747, -1.1026,  ...,  0.2173,  1.2862,  0.7152],\n",
      "        [-0.9411,  1.4792, -0.2581,  ...,  0.4969, -0.1927,  0.2052]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step7\n",
      "Pred=tensor([[-1.0370,  1.9221, -0.1543,  ..., -0.0673,  0.7470,  0.2092],\n",
      "        [-0.6447,  3.4294,  0.0056,  ...,  0.3108,  0.9903,  1.1333],\n",
      "        [-1.1868,  1.7133, -0.5382,  ...,  0.5863,  1.1230,  0.6772],\n",
      "        ...,\n",
      "        [-0.4800,  2.2662, -0.2970,  ...,  0.1657,  0.5431,  0.6592],\n",
      "        [-0.1809,  1.6543, -0.2421,  ...,  0.4178,  1.4306, -0.1022],\n",
      "        [-0.7653,  2.1437,  0.0944,  ..., -0.1021,  0.8365,  0.4656]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step8\n",
      "Pred=tensor([[-3.1077e-01,  1.7699e+00, -2.7247e-01,  ..., -2.7197e-01,\n",
      "          6.1178e-01,  6.5007e-01],\n",
      "        [-9.7498e-01,  1.2316e+00,  2.2859e-01,  ..., -3.5494e-01,\n",
      "          2.6590e-01,  1.8161e-01],\n",
      "        [-7.3746e-01,  1.9303e+00, -1.7950e-01,  ..., -4.4153e-01,\n",
      "          9.8061e-01,  2.7312e-01],\n",
      "        ...,\n",
      "        [-1.1835e+00,  7.9699e-01, -3.0284e-01,  ...,  1.2262e-01,\n",
      "          9.0493e-01,  9.3318e-01],\n",
      "        [ 1.7502e-03,  2.2605e+00,  8.5056e-01,  ..., -2.4529e-01,\n",
      "          5.1588e-01,  1.3257e-01],\n",
      "        [-7.0768e-01,  1.7141e+00, -3.6798e-01,  ..., -4.5832e-01,\n",
      "          1.0149e+00,  1.7264e-01]], grad_fn=<AddmmBackward>)\n",
      "step9\n",
      "Pred=tensor([[-1.3958,  1.8469,  0.3123,  ..., -0.5379,  1.1515,  0.6145],\n",
      "        [-0.9538,  2.1819, -0.4494,  ...,  0.1099, -0.0143,  0.8425],\n",
      "        [-0.4864,  3.0503, -0.1555,  ...,  1.1064,  0.8354,  0.6259],\n",
      "        ...,\n",
      "        [-0.1063,  2.0775, -0.8168,  ...,  0.1109, -0.4799,  0.5097],\n",
      "        [ 0.2082,  3.6048,  1.2527,  ..., -0.5472,  0.0693,  0.7443],\n",
      "        [-0.5343,  1.9778, -0.1096,  ..., -0.1726,  0.0380,  0.3093]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step10\n",
      "Pred=tensor([[-0.4095,  2.8623,  0.0634,  ...,  0.4180,  0.9901,  0.2165],\n",
      "        [-0.4081,  3.1257,  0.3523,  ...,  0.6474,  1.0071, -0.0268],\n",
      "        [-0.2354,  2.4309,  0.2725,  ...,  0.3019,  0.4341,  0.1911],\n",
      "        ...,\n",
      "        [-0.7031,  2.3804,  0.5940,  ...,  0.0754,  0.3799,  1.0453],\n",
      "        [-1.0651,  2.1217, -0.1374,  ...,  0.5090,  0.9810, -0.3162],\n",
      "        [-0.1990,  3.3329, -0.0554,  ...,  1.0338,  0.7484,  0.4963]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step11\n",
      "Pred=tensor([[ 0.5782,  1.7768,  1.3038,  ..., -0.2641, -0.7239,  0.6614],\n",
      "        [ 1.7661,  2.9450,  1.3946,  ...,  0.3442,  0.1470,  0.6125],\n",
      "        [ 0.8333,  2.6953,  2.0111,  ..., -0.3713,  0.7448,  0.0387],\n",
      "        ...,\n",
      "        [ 1.6943,  2.7573,  2.0001,  ..., -0.6370,  0.3378,  0.4525],\n",
      "        [ 0.8267,  3.0109,  0.6677,  ..., -0.3618,  0.7205,  0.5804],\n",
      "        [ 1.7444,  3.3468,  1.0776,  ..., -0.6980,  0.5038,  0.5082]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step12\n",
      "Pred=tensor([[ 1.8910,  5.7782,  1.7087,  ...,  0.3733,  0.0717,  0.0864],\n",
      "        [ 1.2674,  3.9338,  0.5623,  ...,  0.4853,  0.0477,  0.0809],\n",
      "        [ 1.2273,  3.4208,  2.1121,  ...,  0.3626,  0.7285, -0.3129],\n",
      "        ...,\n",
      "        [ 1.3497,  2.7114,  0.9953,  ...,  0.9051,  0.8911,  0.3962],\n",
      "        [ 1.3831,  3.3399,  1.9572,  ...,  0.1449,  0.4233,  0.2733],\n",
      "        [ 1.1252,  3.5802,  0.8924,  ...,  0.2757,  0.2831,  0.1739]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step13\n",
      "Pred=tensor([[ 0.7934,  3.2365,  1.2574,  ...,  0.2274,  0.0931,  0.4649],\n",
      "        [-0.3572,  2.5894,  1.0127,  ..., -0.1254,  0.3538,  0.3618],\n",
      "        [ 0.1678,  3.0696,  1.4453,  ...,  0.3991, -0.0909,  0.4236],\n",
      "        ...,\n",
      "        [ 0.5665,  3.7385,  0.9858,  ...,  0.3033,  0.9802,  0.2582],\n",
      "        [ 0.4493,  2.6873,  0.7207,  ...,  0.2520,  0.0936,  0.3352],\n",
      "        [ 1.1493,  3.6998,  1.7284,  ...,  0.2280, -0.2102,  0.2277]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step14\n",
      "Pred=tensor([[-0.1386,  3.1046,  0.2924,  ...,  0.3605,  0.6148,  0.5791],\n",
      "        [-0.2920,  3.2538,  1.1613,  ...,  0.8304,  0.3551,  0.2366],\n",
      "        [-0.3510,  1.8055,  0.2238,  ...,  0.7965,  0.1434, -0.1253],\n",
      "        ...,\n",
      "        [ 0.8843,  5.6965,  1.1299,  ...,  0.5967,  0.6968,  0.5588],\n",
      "        [-0.5289,  1.9631,  0.8720,  ..., -0.3918,  0.5542,  0.9136],\n",
      "        [-0.2987,  3.3644,  1.0117,  ...,  0.6101,  0.0371,  0.8010]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step15\n",
      "Pred=tensor([[ 3.5982,  3.4363,  3.6433,  ...,  0.2184, -0.5742, -0.0196],\n",
      "        [ 2.7200,  3.7256,  3.6797,  ..., -0.5406, -0.0111,  0.1541],\n",
      "        [ 3.3642,  3.6905,  4.1716,  ...,  0.0917, -0.2835,  1.0783],\n",
      "        ...,\n",
      "        [ 3.0008,  4.7518,  3.4603,  ..., -0.4659, -0.8436,  1.4153],\n",
      "        [ 3.8391,  5.4864,  3.9740,  ...,  0.0965, -0.0755,  0.3116],\n",
      "        [ 2.4892,  3.1688,  2.6570,  ..., -0.0953, -0.1959, -0.4063]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step16\n",
      "Pred=tensor([[ 0.3947,  2.2930,  1.4223,  ..., -0.1388, -0.1537,  0.3062],\n",
      "        [ 1.4643,  4.3670,  1.6501,  ...,  0.1879,  0.1018,  0.2509],\n",
      "        [ 1.0219,  2.9471,  1.2137,  ...,  0.2956,  0.3623, -0.0069],\n",
      "        ...,\n",
      "        [ 1.5758,  4.3482,  2.0862,  ...,  0.9343,  1.0265,  0.2887],\n",
      "        [ 0.8691,  2.8071,  0.5705,  ..., -0.3281,  0.4806,  0.3261],\n",
      "        [ 0.3356,  3.2551,  1.3786,  ..., -0.2953,  0.7403,  0.7605]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step17\n",
      "Pred=tensor([[ 2.3567e+00,  3.1403e+00,  2.5938e+00,  ..., -2.7550e-01,\n",
      "          1.3735e-01, -1.5271e-01],\n",
      "        [ 2.0182e+00,  3.9713e+00,  2.6869e+00,  ..., -1.1187e-01,\n",
      "          1.5555e-02,  4.9262e-01],\n",
      "        [ 2.7552e+00,  4.3740e+00,  2.9048e+00,  ..., -7.5799e-01,\n",
      "          3.1080e-01, -4.8968e-02],\n",
      "        ...,\n",
      "        [ 1.9874e+00,  3.3145e+00,  2.3480e+00,  ..., -9.0730e-01,\n",
      "         -2.0163e-01,  6.2307e-01],\n",
      "        [ 2.1194e+00,  4.0693e+00,  2.6302e+00,  ...,  2.3852e-01,\n",
      "          4.2567e-01, -2.8691e-03],\n",
      "        [ 2.0679e+00,  3.4135e+00,  2.1937e+00,  ...,  7.1316e-02,\n",
      "          6.5225e-02,  5.5782e-01]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step18\n",
      "Pred=tensor([[ 2.9678e-03,  3.8499e+00,  7.4854e-01,  ..., -5.6855e-01,\n",
      "          9.4923e-01,  6.3046e-01],\n",
      "        [ 4.9976e-01,  3.4551e+00,  1.6852e+00,  ...,  3.4551e-01,\n",
      "          4.0110e-01,  1.9010e-01],\n",
      "        [ 5.2233e-01,  3.5349e+00,  1.4863e+00,  ...,  9.2095e-01,\n",
      "          1.3569e-01,  1.6468e-02],\n",
      "        ...,\n",
      "        [ 7.4775e-01,  4.4261e+00,  1.8179e+00,  ...,  2.1364e-01,\n",
      "          1.0969e-01,  1.3880e+00],\n",
      "        [ 1.6931e+00,  3.5366e+00,  1.7849e+00,  ...,  3.2885e-01,\n",
      "          2.4870e-01,  2.9726e-01],\n",
      "        [ 6.5340e-01,  4.6991e+00,  1.4826e+00,  ..., -4.9721e-01,\n",
      "          2.7699e-01,  2.9631e-01]], grad_fn=<AddmmBackward>)\n",
      "step19\n",
      "Pred=tensor([[ 1.6365,  3.6587,  1.9001,  ...,  0.9034,  0.0930,  0.2771],\n",
      "        [ 1.5717,  3.9112,  1.6933,  ...,  0.8182, -0.4192,  0.1047],\n",
      "        [ 0.9736,  3.5619,  2.2516,  ..., -0.2115, -0.5091, -0.1897],\n",
      "        ...,\n",
      "        [ 1.9961,  3.6044,  0.9658,  ...,  0.2728,  0.9044,  0.7088],\n",
      "        [ 2.1563,  4.8936,  1.7797,  ...,  0.4526,  1.1011,  0.6086],\n",
      "        [ 1.7678,  4.5916,  1.1974,  ...,  0.2350,  0.0941, -0.0572]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step20\n",
      "Pred=tensor([[ 2.4935,  4.5762,  3.2601,  ...,  0.1117, -0.3908,  0.3385],\n",
      "        [ 3.1719,  5.7912,  3.4901,  ..., -0.0129,  0.1505, -0.0413],\n",
      "        [ 1.9694,  4.1877,  3.3316,  ..., -0.4540, -0.5381,  0.8792],\n",
      "        ...,\n",
      "        [ 2.4379,  5.4152,  3.8615,  ..., -0.9413,  0.1393,  1.3768],\n",
      "        [ 1.8314,  3.2456,  3.1242,  ..., -0.1332, -0.5969,  0.6016],\n",
      "        [ 1.5462,  4.0728,  3.8164,  ...,  0.0431, -0.1686,  0.1701]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step21\n",
      "Pred=tensor([[ 1.7510,  3.3170,  3.9100,  ..., -0.0938, -0.4067,  0.1419],\n",
      "        [ 3.1992,  4.4879,  4.3619,  ..., -0.2141, -0.6747,  0.1297],\n",
      "        [ 2.3516,  4.3202,  4.2424,  ...,  0.0943, -0.5112,  1.2421],\n",
      "        ...,\n",
      "        [ 2.5984,  5.5096,  3.9124,  ..., -1.1958,  0.1480,  0.9738],\n",
      "        [ 2.3135,  4.4576,  3.5720,  ..., -0.3841, -0.3208,  0.6199],\n",
      "        [ 3.4651,  4.9634,  4.1917,  ...,  0.4224,  0.3593,  0.4152]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step22\n",
      "Pred=tensor([[ 0.1720,  3.1500,  1.1044,  ...,  0.4560, -0.0527,  0.7646],\n",
      "        [ 1.1561,  4.2644,  2.2957,  ...,  0.3216,  0.7141,  0.5501],\n",
      "        [-0.8005,  2.4761,  0.4367,  ..., -0.4272,  0.8154,  0.8267],\n",
      "        ...,\n",
      "        [-0.8518,  3.1491,  1.4073,  ...,  0.6110,  0.7382,  0.2158],\n",
      "        [ 0.5833,  3.2793,  1.5640,  ...,  1.4707,  0.5015,  0.4940],\n",
      "        [ 0.3461,  3.6427,  1.0948,  ...,  0.7115,  1.1906,  0.7605]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step23\n",
      "Pred=tensor([[ 1.1958,  3.6543,  1.3160,  ...,  0.0911,  0.2726,  0.8722],\n",
      "        [ 1.1377,  3.0513,  1.6449,  ...,  0.5008,  0.4005,  0.4392],\n",
      "        [ 1.4983,  3.8526,  1.4327,  ...,  0.5056, -0.2901,  1.4195],\n",
      "        ...,\n",
      "        [ 1.1876,  3.9917,  2.5416,  ..., -0.1737,  0.5261,  0.7983],\n",
      "        [ 1.2116,  4.0577,  1.1117,  ...,  0.1919, -0.4479,  0.5808],\n",
      "        [ 0.9775,  3.9825,  2.2441,  ...,  0.0833, -0.2482,  0.1500]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step24\n",
      "Pred=tensor([[ 1.7038,  4.1309,  3.0981,  ...,  0.2753,  0.5622,  0.7655],\n",
      "        [ 2.0741,  4.8170,  2.6025,  ...,  0.1611,  0.5883,  0.5136],\n",
      "        [ 0.9856,  4.2372,  3.3475,  ...,  0.6668, -0.2331,  0.4797],\n",
      "        ...,\n",
      "        [ 1.7519,  4.2801,  3.1972,  ...,  0.5129, -0.4472,  1.0046],\n",
      "        [ 2.7630,  5.5455,  3.7964,  ...,  0.1784,  0.0710,  0.5673],\n",
      "        [ 1.6205,  4.5521,  2.9790,  ...,  0.1835,  0.6258,  0.5465]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step25\n",
      "Pred=tensor([[ 3.6307,  5.6399,  3.8194,  ...,  0.6985,  0.1101, -0.2112],\n",
      "        [ 3.2431,  5.2104,  4.0944,  ...,  0.1745,  0.5137,  0.8838],\n",
      "        [ 3.4005,  5.8896,  4.1580,  ...,  0.3506, -0.1554,  0.7743],\n",
      "        ...,\n",
      "        [ 2.2988,  4.7692,  3.2812,  ...,  0.4996, -0.1677,  0.0118],\n",
      "        [ 2.6964,  3.9285,  2.4963,  ..., -0.1211, -0.9846,  0.0145],\n",
      "        [ 3.2904,  6.1473,  4.7160,  ...,  0.7351, -0.7587,  0.8810]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step26\n",
      "Pred=tensor([[ 3.2385,  5.6627,  4.4542,  ...,  0.1627,  0.0650,  1.0339],\n",
      "        [ 2.3460,  3.6528,  3.4320,  ...,  0.0346, -0.5536,  0.2672],\n",
      "        [ 3.4554,  5.2928,  4.8720,  ..., -0.3537, -0.9082,  0.6485],\n",
      "        ...,\n",
      "        [ 2.0473,  3.5691,  3.6536,  ..., -0.2681,  0.6613,  0.6278],\n",
      "        [ 4.7041,  7.3013,  4.1891,  ...,  1.2836, -0.4720,  0.7531],\n",
      "        [ 2.6147,  4.9486,  4.7858,  ..., -0.1272, -0.4012,  0.9395]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step27\n",
      "Pred=tensor([[ 1.3360e+00,  2.2861e+00,  1.9068e+00,  ...,  1.3096e-01,\n",
      "         -1.7120e-02,  5.1633e-01],\n",
      "        [ 5.0082e-01,  2.1208e+00,  1.3426e+00,  ...,  1.3005e-01,\n",
      "         -6.2753e-02,  3.2830e-01],\n",
      "        [ 4.6817e-01,  3.3558e+00,  3.5968e+00,  ..., -5.3657e-01,\n",
      "         -7.2081e-01,  1.1086e+00],\n",
      "        ...,\n",
      "        [ 1.7326e+00,  4.2557e+00,  2.1691e+00,  ...,  6.4121e-01,\n",
      "          1.5862e-01,  2.3174e-01],\n",
      "        [ 2.0689e-03,  2.9546e+00,  1.9043e+00,  ...,  7.0645e-01,\n",
      "          3.8114e-01,  3.9064e-01],\n",
      "        [ 2.1075e-01,  2.6437e+00,  1.7401e+00,  ...,  4.9267e-01,\n",
      "          3.5033e-01,  1.1031e+00]], grad_fn=<AddmmBackward>)\n",
      "step28\n",
      "Pred=tensor([[ 3.5996,  6.0630,  5.0532,  ...,  0.1874, -0.4414, -0.2091],\n",
      "        [ 3.1915,  4.3350,  4.2800,  ...,  0.1049, -0.6464, -0.0963],\n",
      "        [ 4.1027,  5.6534,  4.8072,  ...,  0.8251, -0.4438,  0.6413],\n",
      "        ...,\n",
      "        [ 3.9731,  5.6943,  4.5384,  ..., -0.9031, -0.3611,  1.0835],\n",
      "        [ 2.9623,  5.5909,  3.9623,  ..., -0.7223, -1.0482,  0.8818],\n",
      "        [ 2.7122,  4.3993,  3.5488,  ..., -0.6965, -0.0696,  0.1704]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step29\n",
      "Pred=tensor([[ 2.3643,  4.1792,  2.7858,  ...,  0.3642, -0.2610,  0.7764],\n",
      "        [ 2.0988,  3.5162,  2.3647,  ...,  0.3364,  0.4966, -0.5184],\n",
      "        [ 2.5634,  3.9522,  1.4685,  ..., -0.0618,  0.2783, -0.1152],\n",
      "        ...,\n",
      "        [ 0.9652,  3.9010,  1.6529,  ...,  0.8759,  0.0287,  0.7836],\n",
      "        [ 2.0407,  4.4418,  2.1856,  ...,  1.1753, -0.0833,  0.9852],\n",
      "        [ 2.5432,  4.6343,  2.1526,  ..., -0.0084, -0.5460, -0.2334]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step30\n",
      "Pred=tensor([[ 1.7799,  4.4786,  2.9996,  ..., -0.0442,  0.0130,  0.1082],\n",
      "        [ 1.9797,  4.6610,  4.1597,  ...,  0.2043, -0.5857,  0.9774],\n",
      "        [ 2.6120,  5.1128,  2.8147,  ..., -0.1488, -0.0532,  1.6509],\n",
      "        ...,\n",
      "        [ 0.9095,  4.3490,  3.1359,  ...,  0.0904, -0.1419,  1.4092],\n",
      "        [ 2.2026,  5.2367,  4.4944,  ..., -0.1905, -0.5393,  0.4044],\n",
      "        [ 1.2865,  4.0560,  4.0081,  ..., -0.1350, -0.1232,  0.9889]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step31\n",
      "Pred=tensor([[ 3.0577,  4.6699,  4.2906,  ..., -0.3224, -0.3075,  1.3112],\n",
      "        [ 3.8508,  6.2778,  5.7272,  ..., -0.1408, -0.6820,  0.9664],\n",
      "        [ 2.5716,  4.6836,  3.9024,  ...,  0.0306, -0.0975,  0.4289],\n",
      "        ...,\n",
      "        [ 1.5943,  4.3020,  4.9659,  ...,  0.0867, -1.2779,  1.3987],\n",
      "        [ 1.6414,  4.3210,  3.8717,  ..., -0.2098, -0.3115,  1.2412],\n",
      "        [ 4.0103,  4.1159,  4.8537,  ..., -0.0779, -0.3126,  0.5229]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step32\n",
      "Pred=tensor([[ 1.3707,  5.0830,  2.3050,  ...,  0.2000, -0.2390, -0.0169],\n",
      "        [ 1.0650,  4.4704,  1.6963,  ..., -0.1850,  0.4206,  1.6070],\n",
      "        [ 1.5677,  3.9250,  1.8534,  ...,  0.4596,  0.2607,  0.6010],\n",
      "        ...,\n",
      "        [ 1.0495,  4.0139,  2.5781,  ...,  1.5090,  0.4837, -0.1925],\n",
      "        [ 0.5839,  3.8803,  1.7097,  ..., -0.0870, -0.2797,  1.1019],\n",
      "        [ 0.9800,  3.1850,  1.9947,  ...,  0.5117,  0.3006,  0.8555]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step33\n",
      "Pred=tensor([[ 1.4598,  4.9253,  2.4464,  ..., -0.1630,  0.5141,  0.4957],\n",
      "        [ 1.4131,  2.8104,  2.7229,  ...,  0.3303, -0.0224,  0.8393],\n",
      "        [ 0.0154,  3.1494,  2.4278,  ...,  0.0927, -0.3548,  1.1073],\n",
      "        ...,\n",
      "        [ 1.4760,  4.3220,  3.1260,  ...,  0.3395, -0.4090,  1.2152],\n",
      "        [ 0.6390,  3.1253,  1.5366,  ..., -0.6437, -0.5444,  0.4715],\n",
      "        [ 2.7709,  4.7187,  3.2110,  ...,  0.4653, -0.5487,  0.1271]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step34\n",
      "Pred=tensor([[ 2.0143,  3.9764,  1.7413,  ...,  0.1061,  0.6939,  0.1287],\n",
      "        [ 1.7202,  4.3193,  2.4598,  ..., -0.5237, -0.6556,  0.5407],\n",
      "        [ 1.6343,  4.0328,  2.6581,  ...,  0.2525, -0.4722,  1.1165],\n",
      "        ...,\n",
      "        [ 0.9346,  3.3652,  2.5120,  ..., -0.0726, -0.2161,  0.6950],\n",
      "        [ 1.6566,  4.1243,  3.4557,  ...,  0.0448, -0.0518,  0.7149],\n",
      "        [ 1.6774,  3.6175,  2.9917,  ..., -1.0189, -0.1594,  0.4803]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step35\n",
      "Pred=tensor([[ 3.2040e+00,  6.0800e+00,  5.1921e+00,  ..., -1.7961e-01,\n",
      "         -7.8519e-02,  9.0186e-01],\n",
      "        [ 1.7561e+00,  4.6544e+00,  2.8577e+00,  ..., -4.1620e-03,\n",
      "          1.6684e-01,  1.4036e+00],\n",
      "        [ 1.6230e+00,  5.1197e+00,  3.5906e+00,  ..., -3.2592e-01,\n",
      "          2.7449e-01,  1.2684e+00],\n",
      "        ...,\n",
      "        [ 2.5631e+00,  4.6432e+00,  3.3624e+00,  ..., -2.1712e-01,\n",
      "          3.4633e-01,  9.1727e-01],\n",
      "        [ 2.7670e+00,  5.9563e+00,  3.7643e+00,  ..., -1.2215e-01,\n",
      "         -3.7993e-01,  1.5384e+00],\n",
      "        [ 2.2700e+00,  4.1537e+00,  3.1154e+00,  ...,  9.1214e-01,\n",
      "         -8.9476e-01,  3.2113e-01]], grad_fn=<AddmmBackward>)\n",
      "step36\n",
      "Pred=tensor([[ 1.7736,  5.2420,  3.2757,  ...,  0.2218, -0.5041,  0.6926],\n",
      "        [ 2.2670,  4.3984,  3.6950,  ...,  0.3856, -0.9066,  0.4797],\n",
      "        [ 1.7323,  5.0455,  1.9332,  ...,  0.3433, -0.0365,  0.5829],\n",
      "        ...,\n",
      "        [ 1.0703,  4.3984,  2.4970,  ..., -0.3373, -0.2389,  0.2428],\n",
      "        [ 1.7200,  4.0869,  3.5747,  ...,  0.0340, -0.5025,  1.2313],\n",
      "        [ 1.5564,  5.5819,  4.7717,  ...,  0.8339,  0.3599,  1.5994]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step37\n",
      "Pred=tensor([[ 3.3777,  5.4930,  4.9249,  ...,  0.3394, -0.6657,  1.1811],\n",
      "        [ 2.4551,  5.1902,  4.0967,  ..., -0.2004,  0.3348,  1.3582],\n",
      "        [ 1.9930,  4.5788,  4.0901,  ...,  0.3936,  0.1392,  0.9348],\n",
      "        ...,\n",
      "        [ 3.2231,  6.2918,  4.5181,  ...,  0.1206, -0.3006, -0.2259],\n",
      "        [ 3.1306,  6.1712,  5.7075,  ..., -0.0563, -0.2496,  0.7703],\n",
      "        [ 3.1532,  6.7387,  4.1399,  ...,  0.2433, -0.0196,  0.3772]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step38\n",
      "Pred=tensor([[ 1.1303,  4.5551,  1.8912,  ...,  0.2182, -0.4947,  0.9053],\n",
      "        [ 0.7800,  3.6902,  3.1960,  ...,  0.6984,  0.9954,  0.1021],\n",
      "        [ 1.9135,  4.7256,  1.9326,  ...,  1.0076, -0.5997,  0.0630],\n",
      "        ...,\n",
      "        [ 1.5342,  5.9523,  3.9846,  ...,  0.4398, -0.6073,  0.4044],\n",
      "        [ 2.5851,  5.7067,  3.4763,  ...,  0.6416, -0.4086,  0.1049],\n",
      "        [ 1.7970,  5.5626,  2.7523,  ...,  0.1526, -0.2128,  0.9033]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step39\n",
      "Pred=tensor([[ 3.4560,  5.3148,  5.7427,  ..., -0.5032, -0.0813,  0.1681],\n",
      "        [ 3.9429,  5.9158,  4.8873,  ...,  0.0884, -0.3631,  0.6451],\n",
      "        [ 3.1863,  5.4288,  4.5323,  ...,  0.0849, -0.3276,  0.5834],\n",
      "        ...,\n",
      "        [ 3.0881,  4.8634,  4.1500,  ..., -0.4873, -0.2953,  1.1845],\n",
      "        [ 3.2980,  4.8918,  4.4165,  ..., -0.0964, -0.1383,  0.8374],\n",
      "        [ 3.5181,  6.1545,  4.8775,  ...,  0.5645, -0.0444,  0.2751]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step40\n",
      "Pred=tensor([[ 1.6162e+00,  4.6624e+00,  2.8335e+00,  ..., -4.4863e-03,\n",
      "         -3.5169e-01,  4.9227e-01],\n",
      "        [ 2.8998e+00,  5.3861e+00,  3.7780e+00,  ...,  1.0712e+00,\n",
      "         -2.6006e-02,  4.1769e-01],\n",
      "        [ 1.5142e+00,  4.3273e+00,  3.0976e+00,  ..., -2.2511e-01,\n",
      "         -6.6884e-01,  8.6532e-01],\n",
      "        ...,\n",
      "        [ 2.4523e+00,  5.7510e+00,  4.0004e+00,  ...,  7.8794e-02,\n",
      "         -1.2229e-01,  1.9005e+00],\n",
      "        [ 2.6836e+00,  4.7436e+00,  2.6704e+00,  ...,  1.8500e-01,\n",
      "         -2.7658e-01,  5.4202e-01],\n",
      "        [ 2.8380e+00,  4.8843e+00,  3.1988e+00,  ...,  5.8144e-01,\n",
      "          2.7555e-01,  6.5001e-01]], grad_fn=<AddmmBackward>)\n",
      "step41\n",
      "Pred=tensor([[ 2.6674,  5.0834,  3.6576,  ...,  0.4939, -0.5731,  0.6730],\n",
      "        [ 1.1586,  5.5853,  3.0335,  ...,  0.5540, -0.8092,  0.9015],\n",
      "        [ 2.0163,  4.3042,  2.0603,  ...,  0.9649, -0.2449,  0.5427],\n",
      "        ...,\n",
      "        [ 2.8934,  4.3643,  3.3035,  ...,  0.1931,  0.0261,  0.4756],\n",
      "        [ 1.4415,  4.3352,  2.5669,  ...,  0.3918, -0.2149,  0.4967],\n",
      "        [ 1.4089,  4.3826,  2.9849,  ...,  0.4528,  0.7348,  0.4308]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step42\n",
      "Pred=tensor([[ 3.0975,  6.2060,  3.2495,  ...,  0.8254, -0.8816,  1.0147],\n",
      "        [ 1.8232,  4.1708,  3.0832,  ..., -0.7105, -0.4237,  1.1360],\n",
      "        [ 3.3772,  6.0814,  4.0605,  ...,  0.7118, -0.3055,  0.6998],\n",
      "        ...,\n",
      "        [ 2.5371,  6.1611,  4.3198,  ...,  0.5092,  0.0173,  1.1134],\n",
      "        [ 3.0519,  5.7321,  5.4719,  ..., -0.2934, -0.5759,  1.2403],\n",
      "        [ 2.0019,  5.4479,  4.5969,  ..., -0.3436, -0.6698,  0.8240]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step43\n",
      "Pred=tensor([[ 2.7806,  5.7505,  3.5329,  ...,  0.3016, -0.8874,  1.1938],\n",
      "        [ 3.0679,  4.9774,  3.7215,  ..., -0.3198, -1.4862,  1.2518],\n",
      "        [ 3.2879,  5.3687,  4.7738,  ...,  0.9198,  0.3657,  0.0184],\n",
      "        ...,\n",
      "        [ 3.3908,  5.4274,  4.4320,  ...,  0.4756, -0.8006,  0.7273],\n",
      "        [ 2.3316,  4.7168,  3.8562,  ...,  0.0862,  0.3810,  0.9701],\n",
      "        [ 2.9917,  5.3752,  3.7625,  ..., -0.0810,  0.4886,  0.8402]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step44\n",
      "Pred=tensor([[ 2.7042,  5.8018,  3.6382,  ...,  0.1036,  0.3161,  0.8367],\n",
      "        [ 3.3863,  5.2315,  3.8511,  ...,  0.4854,  0.7231, -0.0562],\n",
      "        [ 2.3906,  4.7020,  4.3338,  ...,  0.1317,  0.2327,  0.7670],\n",
      "        ...,\n",
      "        [ 2.3729,  7.0936,  3.2274,  ...,  1.0689,  0.3255,  0.5782],\n",
      "        [ 3.1567,  6.6501,  4.3718,  ..., -0.2845, -0.1569, -0.5200],\n",
      "        [ 4.8580,  5.8745,  3.7075,  ...,  1.6650,  0.5312,  0.3364]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "step45\n",
      "Pred=tensor([[ 3.1961,  5.6508,  5.3401,  ..., -0.3411, -0.9642,  0.7524],\n",
      "        [ 4.3224,  6.9495,  4.7073,  ...,  0.3576, -0.5389,  1.1267],\n",
      "        [ 3.5661,  5.6343,  5.9061,  ..., -0.2196,  0.0727,  0.2839],\n",
      "        ...,\n",
      "        [ 4.4685,  4.6177,  4.9048,  ..., -0.2631, -0.9742,  0.4813],\n",
      "        [ 5.0568,  6.0127,  4.0415,  ..., -0.0796, -0.7873,  0.6831],\n",
      "        [ 3.9572,  7.1166,  5.0108,  ...,  0.0963, -0.8770,  0.4480]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-34cb004b2829>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_ft2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-48cde532aa55>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# index番目の画像をロード\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [高さ][幅][色RGB]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# 画像の前処理を実施\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2911\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2913\u001b[1;33m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2915\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#練習用\n",
    "lossfunc = loss_func\n",
    "for epoch in range(1, 5+1):\n",
    "        running_loss = 0\n",
    "        running_train_loss = 0\n",
    "        running_test_loss = 0\n",
    "\n",
    "        # 学習モード\n",
    "        net = model_ft2\n",
    "        net.train()\n",
    "        for step, batch  in enumerate(train_loader, 1):\n",
    "            print('step{}'.format(step))\n",
    "            net.train()\n",
    "            xx, yy = batch[0].to(device), batch[1].to(device)\n",
    "            xx = xx/255.0 - 0.5\n",
    "           # 最後に計算した各パラメーターの勾配を初期化する\n",
    "            optimizer.zero_grad()\n",
    "            # フォワード計算を行う\n",
    "            y_pred = net(xx)\n",
    "            # 誤差関数を使ってギャップの計測\n",
    "            loss = lossfunc(y_pred, yy)\n",
    "            #print('X={}'.format(xx))\n",
    "            print('Pred={}'.format(y_pred))\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            # 誤差逆伝播法を使って自動微分\n",
    "            loss.backward()\n",
    "            # パラメーターを更新\n",
    "            optimizer.step()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "# False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'trainloader': train_loader,\n",
    "    'valloader': test_loader,\n",
    "    'net': model_ft2,\n",
    "    'optimizer': optimizer,\n",
    "    'lossfunc': loss_func,\n",
    "    'epochs': 5,\n",
    "    'device': device\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(trainloader, valloader, net, optimizer, lossfunc, epochs, device):\n",
    "    train_loss_track = []\n",
    "    test_loss_track = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        running_loss = 0\n",
    "        running_train_loss = 0\n",
    "        running_test_loss = 0\n",
    "\n",
    "        # 学習モード\n",
    "        net.train()\n",
    "        for step, batch  in enumerate(trainloader, 1):\n",
    "            # device = \"cuda\"の場合、GPUにデータを転送する\n",
    "            xx, yy = batch[0].to(device), batch[1].to(device), #batchのtensor の中でX,yという配列で格納されている\n",
    "            #print('xx{}'.format(xx))\n",
    "            #print('yy{}'.format(yy))\n",
    "            #xx, yy = batch[0], batch[1]\n",
    "            \n",
    "            #prev_model = model_ft2\n",
    "            #rev_optimizer = optimizer\n",
    "            \n",
    "            # 最後に計算した各パラメーターの勾配を初期化する\n",
    "            optimizer.zero_grad()\n",
    "            # フォワード計算を行う\n",
    "            y_pred = net(xx)\n",
    "            # 誤差関数を使ってギャップの計測\n",
    "            loss = lossfunc(y_pred, yy)\n",
    "            print('prediction{}'.format(y_pred))\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            # 誤差逆伝播法を使って自動微分\n",
    "            loss.backward()\n",
    "            # パラメーターを更新\n",
    "            optimizer.step()\n",
    "            # 学習データを使って損失を計算\n",
    "            running_loss += loss.item()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # 100 iterationごとにlossを出力\n",
    "            if step % 100 == 0:\n",
    "                print('[{:d} epoch, {:5d} iter] train/loss: {}'.format(epoch, step, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "        # エポックが終了したら平均損失を計算\n",
    "        train_loss_track.append(running_train_loss / len(train_loader))\n",
    "\n",
    "        # 評価（evaluation）モード\n",
    "        net.eval()\n",
    "        # 勾配計算用のパラメータを保存しない\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(valloader, 1):\n",
    "                # device = \"cuda\"の場合、GPUにデータを転送する\n",
    "                xx_test, yy_test = batch[0].to(device), batch[1].to(device)\n",
    "                #xx_test, yy_test = batch[0], batch[1]\n",
    "                # 予測値を計算\n",
    "                y_pred = net(xx_test)\n",
    "                # 誤差関数を使ってギャップの計測\n",
    "                test_loss = lossfunc(y_pred, yy_test)\n",
    "                # テストデータを使って損失を計算\n",
    "                running_test_loss += test_loss.item()\n",
    "        # 誤差をトラッキング\n",
    "        test_loss_track.append(running_test_loss / len(valloader))\n",
    "        print('val/loss: {}'.format(running_test_loss / len(valloader)))\n",
    "    return net, train_loss_track, test_loss_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictiontensor([[ 0.1718,  1.5973,  3.6517,  ..., -0.5247, -1.1722,  1.4372],\n",
      "        [-0.7109,  1.6782, -0.2509,  ..., -3.3883, -0.9128,  1.3525],\n",
      "        [ 1.0328,  2.6343,  0.3135,  ..., -0.4509,  0.2642,  3.3603],\n",
      "        ...,\n",
      "        [ 1.3575,  2.8012,  3.4027,  ..., -0.9443, -0.5447,  1.7857],\n",
      "        [ 0.6019,  1.5006,  2.0185,  ..., -3.0108, -0.4060,  2.5505],\n",
      "        [-3.5004, -0.8450, -1.2210,  ..., -1.7256, -2.6423,  4.0673]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 0.8030,  1.6257,  3.5304,  ..., -2.2327,  1.6124, -0.6394],\n",
      "        [ 0.5029,  1.7032,  1.7462,  ..., -2.6410,  0.6951, -2.1919],\n",
      "        [ 1.9527,  2.0926,  2.0758,  ..., -2.8828,  0.3324, -1.5571],\n",
      "        ...,\n",
      "        [-0.1709,  2.1385,  2.2761,  ..., -2.9661, -0.5804, -1.6920],\n",
      "        [ 0.3188,  0.9138,  0.5355,  ..., -2.8859,  1.0315, -2.2853],\n",
      "        [ 0.1958,  1.8650,  1.3048,  ..., -2.2152,  0.5784, -2.4533]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 6.7271,  7.8893,  6.7990,  ..., -0.9099,  1.2881,  0.0890],\n",
      "        [ 4.9132,  5.8521,  3.7459,  ...,  2.1082,  0.1112, -0.6965],\n",
      "        [ 6.8142,  6.3274,  4.2871,  ...,  4.2741,  0.3611, -0.2765],\n",
      "        ...,\n",
      "        [ 6.1468,  8.1436,  4.5734,  ...,  3.0375, -0.1975,  0.2696],\n",
      "        [ 6.8498,  7.3791,  5.7714,  ...,  2.4061, -0.6478,  0.1572],\n",
      "        [ 6.7529,  7.9832,  3.8140,  ...,  2.1201,  1.7405, -1.9032]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 3.0559,  5.6356,  6.4983,  ..., -3.4278, -1.5987, -1.5252],\n",
      "        [ 5.3498,  6.2528,  8.5088,  ...,  0.9970,  1.2139, -0.1280],\n",
      "        [ 4.5769,  4.8706,  9.2310,  ...,  0.3849,  0.7671, -0.1427],\n",
      "        ...,\n",
      "        [ 5.1920,  6.0565,  8.7191,  ..., -0.1440,  0.4638, -0.3690],\n",
      "        [ 5.4179,  5.2767,  9.1841,  ..., -0.5384, -0.2653,  0.9058],\n",
      "        [ 6.4394,  6.8611,  8.5251,  ...,  0.1587, -0.3632,  0.8032]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[-2.9973, -0.3765, -0.9204,  ..., -3.0838,  0.4680,  1.8416],\n",
      "        [-1.6862, -0.9485, -0.8774,  ..., -1.3967,  0.3022,  1.5363],\n",
      "        [-2.2129, -1.5215, -2.6472,  ..., -2.2817,  0.8748,  0.6786],\n",
      "        ...,\n",
      "        [-0.9798, -0.6893, -0.5492,  ..., -1.6894, -0.5681,  0.7359],\n",
      "        [-1.2313, -1.2453, -0.5683,  ..., -2.4799, -0.3882,  1.8977],\n",
      "        [-1.8360, -1.7465, -0.7226,  ..., -2.9225, -0.3562,  4.1615]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[-0.8912, -0.2246, -0.6367,  ..., -1.2952,  1.7373,  0.0787],\n",
      "        [-1.1165,  0.0402, -1.4021,  ..., -0.4518,  1.0849, -1.3852],\n",
      "        [-1.6180, -0.7331, -0.5466,  ..., -1.2026,  1.0549,  0.2123],\n",
      "        ...,\n",
      "        [-1.5605, -0.3582, -0.7691,  ..., -1.5323,  1.2176, -0.2972],\n",
      "        [-1.6196, -0.6339, -0.6348,  ..., -1.4403, -0.2541,  0.0778],\n",
      "        [-1.4898, -0.1724, -0.0342,  ..., -1.1928,  0.1997,  0.1983]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 0.2572,  1.1401,  0.6256,  ..., -0.6899,  0.5559,  0.2540],\n",
      "        [-0.9106, -0.1603,  0.1960,  ..., -1.2271, -0.2921,  0.5017],\n",
      "        [-0.6508,  0.0017, -0.0697,  ..., -0.3935, -0.3423,  0.4627],\n",
      "        ...,\n",
      "        [-0.0702,  0.8894,  0.5488,  ..., -0.7748,  0.3960,  0.3919],\n",
      "        [-0.2872,  0.5838, -0.4779,  ..., -0.7326,  0.4670,  0.0919],\n",
      "        [-0.1061,  0.3131,  0.2395,  ..., -0.5942,  0.0218,  0.4384]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[-0.4628,  0.1009, -0.1947,  ..., -0.7195,  1.1870, -0.0055],\n",
      "        [-0.9670,  0.5108,  0.4175,  ..., -0.2238,  0.2195,  0.1635],\n",
      "        [ 0.2896,  1.6403,  0.1886,  ..., -1.2108,  0.5938,  0.1100],\n",
      "        ...,\n",
      "        [-0.4901,  0.9205,  0.6041,  ...,  0.4860,  0.5923, -0.2206],\n",
      "        [ 1.1370,  1.5347,  0.7871,  ..., -0.1605,  1.0305,  0.0841],\n",
      "        [ 0.2643,  0.9502,  1.3345,  ...,  0.2352,  0.1000, -0.2659]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 1.3471,  6.1364,  2.6988,  ...,  1.2078,  0.1455,  1.1032],\n",
      "        [ 3.9270,  7.4445,  3.7707,  ..., -0.5682,  1.1313, -0.2179],\n",
      "        [ 4.4415,  6.4087,  1.4382,  ...,  3.7858, -1.6346,  0.4746],\n",
      "        ...,\n",
      "        [ 2.5283,  5.5181,  2.8688,  ...,  0.8019, -1.3403, -0.0112],\n",
      "        [ 1.5366,  5.1476,  2.8049,  ...,  2.2027,  1.4545, -0.2575],\n",
      "        [ 1.0512,  4.5483,  2.4960,  ...,  0.5538,  0.3382,  1.4151]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[-0.4764,  0.6710,  0.1193,  ..., -0.5390, -0.1753,  0.4735],\n",
      "        [ 0.0514,  1.2986,  0.9085,  ..., -0.2012,  0.1148,  0.3148],\n",
      "        [ 0.1674,  1.4091,  1.1430,  ..., -0.2750,  0.4108,  0.3368],\n",
      "        ...,\n",
      "        [-0.3552,  0.5239,  0.2357,  ...,  0.3362, -0.2241,  0.4751],\n",
      "        [-0.5555,  1.0505,  0.5400,  ..., -0.0745, -0.0664,  0.6181],\n",
      "        [-0.1140,  0.8036,  1.6192,  ..., -0.3787, -0.3188,  0.5737]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 8.6355e-02,  3.0632e+00,  1.3475e+00,  ...,  1.4476e+00,\n",
      "          8.8924e-01,  3.6483e-01],\n",
      "        [ 1.7230e+00,  4.8765e+00,  2.5634e+00,  ...,  1.9148e+00,\n",
      "         -8.6597e-01,  2.1104e+00],\n",
      "        [ 3.7499e-03,  3.6461e+00,  1.3807e-01,  ...,  5.3321e-01,\n",
      "          6.3166e-01,  1.0449e+00],\n",
      "        ...,\n",
      "        [ 1.9302e+00,  4.3232e+00,  1.6575e+00,  ...,  1.4594e+00,\n",
      "         -6.9075e-01,  2.5289e+00],\n",
      "        [ 1.9808e+00,  6.2839e+00,  1.8202e+00,  ...,  1.8910e+00,\n",
      "         -1.6350e+00,  1.2405e+00],\n",
      "        [ 3.6436e-01,  5.2667e+00,  2.8802e+00,  ...,  3.0467e-01,\n",
      "         -1.8322e+00,  2.9398e+00]], grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[-0.3420,  2.2387,  3.4701,  ...,  0.6162,  0.2151,  1.6363],\n",
      "        [ 0.4355,  2.9491,  2.7045,  ...,  1.2606,  0.3771,  1.5520],\n",
      "        [ 0.6131,  4.5317,  3.3272,  ..., -0.2807,  0.0954,  2.1486],\n",
      "        ...,\n",
      "        [-0.0074,  3.4370,  4.0618,  ...,  1.0443, -0.3882,  1.8628],\n",
      "        [ 0.7551,  3.7399,  3.6072,  ...,  0.9051,  0.6511,  1.9778],\n",
      "        [ 0.9754,  3.6741,  4.1657,  ..., -0.4065, -1.0665,  1.6641]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 0.8351,  2.5084,  3.8309,  ..., -0.2146,  0.4482,  1.0053],\n",
      "        [ 1.1542,  4.8191,  3.2312,  ..., -0.1460,  1.0639,  2.2594],\n",
      "        [ 2.1823,  4.2022,  4.6145,  ...,  0.5888,  0.3512,  1.2863],\n",
      "        ...,\n",
      "        [ 3.1543,  5.4222,  4.5997,  ...,  0.0278,  0.1737,  1.4062],\n",
      "        [ 2.1415,  4.6982,  4.4501,  ...,  1.5640,  1.6889,  1.2482],\n",
      "        [ 1.2620,  2.3884,  2.7140,  ..., -0.0596, -0.1662,  1.4981]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 4.0468,  3.6568,  3.8776,  ...,  1.2494, -2.2017,  0.6551],\n",
      "        [ 2.0026,  3.5638,  4.3592,  ...,  0.8115, -0.9714,  0.0519],\n",
      "        [ 3.2620,  4.5379,  4.0010,  ...,  0.2926, -1.3322,  0.1816],\n",
      "        ...,\n",
      "        [ 2.4824,  4.4649,  4.3424,  ...,  1.5682, -0.8348, -0.6018],\n",
      "        [ 3.0947,  4.0658,  2.0754,  ...,  1.4408,  0.6073,  1.0326],\n",
      "        [ 1.7419,  1.8578,  2.0442,  ...,  1.8573,  0.9523, -1.4250]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 2.5292,  4.8192,  3.6250,  ...,  1.1770,  0.5043,  1.3725],\n",
      "        [ 3.2680,  6.4325,  4.3549,  ...,  2.6265, -1.2930,  0.8859],\n",
      "        [ 2.6937,  4.5018,  4.3832,  ...,  0.8825,  0.2226,  1.5560],\n",
      "        ...,\n",
      "        [ 3.1868,  3.9361,  5.9831,  ...,  0.7282, -1.1426,  1.9394],\n",
      "        [ 2.9777,  5.4690,  4.7450,  ...,  2.7029, -0.4174,  1.5851],\n",
      "        [ 2.9478,  3.9043,  5.2247,  ...,  0.5802, -0.7926,  1.0763]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 3.2918,  4.6523,  5.9454,  ...,  0.4158,  0.8492,  1.2399],\n",
      "        [ 2.1952,  6.3612,  4.2564,  ..., -0.0916,  0.1052,  1.2278],\n",
      "        [ 2.3772,  5.0723,  3.9807,  ...,  0.4643, -0.0233,  2.0994],\n",
      "        ...,\n",
      "        [ 2.5223,  6.3921,  3.4326,  ...,  1.4250,  0.0301,  1.1084],\n",
      "        [ 1.4119,  4.4973,  1.9001,  ...,  1.1843,  0.8574,  0.9175],\n",
      "        [ 2.4253,  5.0206,  4.6772,  ...,  2.1362, -0.1039,  1.4995]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 2.5774,  5.3236,  3.1486,  ...,  1.4109,  0.3900,  1.1223],\n",
      "        [ 2.1563,  4.8143,  2.5781,  ...,  0.5078, -0.3478,  0.8939],\n",
      "        [ 4.2254,  5.9865,  3.8964,  ...,  1.1540,  0.3349,  0.2991],\n",
      "        ...,\n",
      "        [ 3.6304,  6.1977,  3.6382,  ...,  2.9474,  0.4819,  1.1082],\n",
      "        [ 5.1241,  5.9993,  6.7103,  ...,  2.5262,  0.1428,  0.4009],\n",
      "        [ 4.9129,  5.9862,  4.9862,  ...,  1.7461,  0.3505,  0.2521]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictiontensor([[ 3.8567,  7.8820,  4.7903,  ...,  0.9702,  0.6170,  1.4037],\n",
      "        [ 4.5524,  7.9428,  7.1001,  ...,  1.2726,  0.5620,  0.8046],\n",
      "        [ 2.5715,  5.5840,  5.3133,  ...,  0.2613,  0.0101,  1.8276],\n",
      "        ...,\n",
      "        [ 3.4829,  5.7726,  4.3763,  ...,  1.6317,  0.3305,  1.6551],\n",
      "        [ 2.4527,  7.4752,  4.5779,  ...,  1.1570, -0.6792,  1.1433],\n",
      "        [ 1.8161,  4.8056,  3.5366,  ...,  1.1326, -0.2686,  0.9589]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 4.1827,  5.9679,  3.0886,  ...,  0.6385,  0.2729,  0.2595],\n",
      "        [ 2.4416,  5.1052,  4.7078,  ..., -0.9333,  0.1902,  0.8431],\n",
      "        [ 6.1815,  9.8610,  4.7265,  ...,  1.3164,  0.1508,  0.4958],\n",
      "        ...,\n",
      "        [ 3.0326,  6.4338,  4.4601,  ...,  0.4591, -0.1807, -0.5072],\n",
      "        [ 2.8569,  6.0742,  6.1112,  ..., -0.0120, -1.1586,  1.4014],\n",
      "        [ 4.4502,  6.1745,  4.8814,  ..., -0.0881, -0.0387,  1.1055]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 5.0821,  6.4779,  4.7034,  ...,  0.6005, -0.2063,  1.3007],\n",
      "        [ 5.3000,  8.7745,  6.1500,  ...,  0.1983, -0.7287,  1.2566],\n",
      "        [ 4.1847,  5.3663,  5.3061,  ...,  1.0899,  0.9249,  1.0260],\n",
      "        ...,\n",
      "        [ 3.0322,  5.7834,  5.3249,  ..., -0.6992, -0.6460,  0.3473],\n",
      "        [ 4.4613,  7.6229,  7.1534,  ..., -0.4401, -0.0925,  1.1075],\n",
      "        [ 2.9597,  5.3482,  8.2962,  ...,  0.9072, -0.6746,  1.2518]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 3.0009,  5.2844,  3.6344,  ...,  0.3624, -1.5708,  0.8902],\n",
      "        [ 2.4405,  5.2233,  3.9778,  ...,  1.1907, -0.4812,  1.5400],\n",
      "        [ 3.3389,  5.2791,  4.1771,  ..., -0.3781, -0.1596,  0.2268],\n",
      "        ...,\n",
      "        [ 3.3006,  5.0097,  4.4889,  ...,  0.7569, -0.8565,  1.9231],\n",
      "        [ 3.3264,  8.0229,  3.9151,  ..., -0.3176,  0.3873,  1.7041],\n",
      "        [ 3.1846,  5.5811,  3.4524,  ..., -0.3517,  0.4105,  2.3013]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predictiontensor([[ 4.0438,  5.2202,  5.3828,  ...,  0.2496,  0.6034,  1.0595],\n",
      "        [ 2.6725,  4.9563,  5.6088,  ...,  0.1164,  0.3099, -0.0088],\n",
      "        [ 5.3745,  6.2503,  5.3583,  ...,  1.1702, -0.5878,  1.4211],\n",
      "        ...,\n",
      "        [ 3.6932,  5.2876,  4.0584,  ...,  2.0755,  0.7891,  0.2976],\n",
      "        [ 4.3388,  5.9676,  5.5133,  ...,  1.3350,  0.4162,  0.8200],\n",
      "        [ 4.1233,  6.6193,  5.6793,  ...,  0.7187,  0.6764,  0.6816]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-569dded416f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_ft2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss_track\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss_track\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-c4ac1602b816>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(trainloader, valloader, net, optimizer, lossfunc, epochs, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m#optimizer.zero_grad()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# 誤差逆伝播法を使って自動微分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[1;31m# パラメーターを更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft2, train_loss_track, test_loss_track = training(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1000, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交差エントロピー誤差の推移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_loss_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(testloader, net):\n",
    "    true = 0\n",
    "    total = 0\n",
    "\n",
    "    all_labels = np.array([])\n",
    "    all_preds = np.array([])\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_xx, test_yy in testloader:\n",
    "            \n",
    "            # device = \"cuda\"の場合、GPUにデータを転送する\n",
    "            test_xx = test_xx.to(device)\n",
    "            test_yy = test_yy.to(device)\n",
    "\n",
    "            outputs = net(test_xx)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_labels = np.append(all_labels, test_yy.cpu().data.numpy())\n",
    "            all_preds = np.append(all_preds, predicted.cpu().numpy())\n",
    "            \n",
    "            total += test_yy.size(0)\n",
    "            true += (predicted == test_yy).sum().item()\n",
    "    print('Accuracy: {:.2f} %'.format(100 * float(true/total)))\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels, all_preds = calculate_accuracy(test_loader, model_ft2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testデータで混同行列を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(all_labels, all_preds):\n",
    "    labels = np.unique(all_labels)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=labels)\n",
    "    cm_labeled = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "    return cm_labeled\n",
    "\n",
    "plot_conf_mat(all_labels, all_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
